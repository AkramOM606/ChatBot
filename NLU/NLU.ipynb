{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intent Recognition & Context Management Integration\n",
    "\n",
    "*This notebook integrates existing text cleaning and processing pipeline with intent recognition using Word2Vec-style embeddings (from the pre-trained GloVe model) and context management. We use an expanded dataset of common intents to train a classifier. The predicted intent and updated context for each input are printed without generating a response.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\asusg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\asusg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\asusg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "import yaml\n",
    "import gensim.downloader as api\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "# Load pre-trained GloVe model (using gensim)\n",
    "pretrained_model = api.load(\"glove-wiki-gigaword-100\")\n",
    "english_stop_words = list(set(stopwords.words(\"english\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Pre-Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lowercase(prompt):\n",
    "    return prompt.lower()\n",
    "\n",
    "def delete_stopwords(prompt):\n",
    "    return \" \".join([word for word in prompt.split() if word not in english_stop_words])\n",
    "\n",
    "def text_cleaning(prompt):\n",
    "    ignore_character = list(string.punctuation)\n",
    "    pattern = f\"[{re.escape(''.join(ignore_character))}]\"\n",
    "    cleaned_prompt = re.sub(pattern, \" \", prompt)\n",
    "    cleaned_prompt = re.sub(r\"\\b[a-z]\\b\", \"\", cleaned_prompt)\n",
    "    cleaned_prompt = re.sub(r\"\\s+\", \" \", cleaned_prompt)\n",
    "    return cleaned_prompt.strip()\n",
    "\n",
    "def tokenization(sentence):\n",
    "    return word_tokenize(sentence)\n",
    "\n",
    "def lemmatization(tokens):\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "def nettoyage_corpus(corpus):\n",
    "    \"\"\"\n",
    "    Process a list of documents: lowercasing, stopword deletion,\n",
    "    cleaning, tokenization, and lemmatization.\n",
    "    \"\"\"\n",
    "    cleaned_conversations = [\n",
    "        lemmatization(tokenization(text_cleaning(delete_stopwords(to_lowercase(doc)))))\n",
    "        for doc in corpus\n",
    "    ]\n",
    "    return cleaned_conversations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence Embedding Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_embedding(model, sentence):\n",
    "    tokens = word_tokenize(sentence.lower())\n",
    "    valid_tokens = [token for token in tokens if token in model]\n",
    "    if not valid_tokens:\n",
    "        return np.zeros(model.vector_size)\n",
    "    embeddings = [model[token] for token in valid_tokens]\n",
    "    return np.mean(embeddings, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expanded Intent Dataset and Classifier Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_phrases = {\n",
    "    \"greeting\": [\n",
    "        \"Hello\", \"Hi\", \"Hey there\", \"Good morning\", \"Good afternoon\", \"Good evening\",\n",
    "        \"What's up\", \"Greetings\", \"Howdy\", \"Hi, how are you?\", \"Hey\", \"Hello there\",\n",
    "        \"Hey, what's going on?\", \"Yo\", \"Hiya\", \"Hello, nice to see you!\", \"Hey buddy\",\n",
    "        \"Good to see you\", \"Hi, hope you're well\", \"Hello, how do you do?\"\n",
    "    ],\n",
    "    \"goodbye\": [\n",
    "        \"Goodbye\", \"Bye\", \"See you later\", \"Talk to you soon\", \"Farewell\", \"Take care\",\n",
    "        \"Catch you later\", \"See ya\", \"Bye bye\", \"Adios\", \"Later\", \"So long\", \"Good night\",\n",
    "        \"I'm off\", \"Peace out\", \"Ciao\", \"Until next time\", \"Farewell for now\",\n",
    "        \"See you around\", \"Later alligator\"\n",
    "    ],\n",
    "    \"get_time\": [\n",
    "        \"What time is it?\", \"Tell me the current time\", \"Could you give me the time?\",\n",
    "        \"I need to know the time\", \"Time please\", \"Do you know what time it is?\",\n",
    "        \"Can you tell me the time?\", \"What's the time now?\", \"Please share the time\",\n",
    "        \"Current time?\", \"Time update\", \"What's the clock saying?\", \"Show me the time\",\n",
    "        \"Time check\", \"What's the time, please?\", \"May I know the time?\",\n",
    "        \"Could you update me with the time?\", \"Time now?\", \"Let me know the time\", \"Time?\"\n",
    "    ],\n",
    "    \"get_weather\": [\n",
    "        \"What's the weather like today?\", \"Tell me the weather forecast\", \"How is the weather?\",\n",
    "        \"Is it going to rain?\", \"Weather update please\", \"What's the temperature outside?\",\n",
    "        \"Do I need an umbrella today?\", \"Weather report\", \"Current weather conditions?\",\n",
    "        \"How's the weather outside?\", \"Forecast for today?\", \"Is it sunny or rainy?\",\n",
    "        \"Weather status\", \"What's the climate like today?\", \"Do I need a jacket today?\",\n",
    "        \"How's the weather looking?\", \"Any rain expected today?\", \"Weather check\",\n",
    "        \"Let me know today's weather\", \"Weather update\"\n",
    "    ],\n",
    "    \"thanks\": [\n",
    "        \"Thank you\", \"Thanks a lot\", \"Much appreciated\", \"Thanks\", \"Thank you very much\",\n",
    "        \"I appreciate it\", \"Thanks a million\", \"Thank you so much\", \"Cheers\", \"Thanks a bunch\",\n",
    "        \"Many thanks\", \"I'm grateful\", \"Thank you kindly\", \"I owe you one\", \"Appreciate it\",\n",
    "        \"Thanks for everything\", \"Thanks, that was helpful\", \"Thank you, really appreciate it\",\n",
    "        \"Thanks a ton\", \"Sincere thanks\"\n",
    "    ],\n",
    "    \"apology\": [\n",
    "        \"I'm sorry\", \"My apologies\", \"Sorry for that\", \"I apologize\", \"Please forgive me\",\n",
    "        \"Sorry about that\", \"My bad\", \"I didn't mean that\", \"I am really sorry\", \"Apologies\",\n",
    "        \"I regret that\", \"So sorry\", \"Excuse me\", \"Pardon me\", \"I beg your pardon\",\n",
    "        \"I sincerely apologize\", \"Forgive me, please\", \"I apologize for any inconvenience\",\n",
    "        \"I'm truly sorry\", \"Sorry, my mistake\"\n",
    "    ],\n",
    "    \"unknown\": [\n",
    "        \"I don't know\", \"Can you repeat that?\", \"What do you mean?\", \"I don't understand\",\n",
    "        \"Could you say that again?\", \"Not sure what you mean\", \"I'm confused\", \"What?\",\n",
    "        \"Huh?\", \"I have no idea\", \"Could you clarify?\", \"I didn't catch that\",\n",
    "        \"Sorry, what did you say?\", \"I am not sure I follow\", \"Please explain\",\n",
    "        \"I don't follow\", \"Could you rephrase that?\", \"I don't comprehend\", \"Unclear to me\", \"Not sure\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "texts = []\n",
    "labels = []\n",
    "for intent, phrases in intent_phrases.items():\n",
    "    for phrase in phrases:\n",
    "        texts.append(phrase)\n",
    "        labels.append(intent)\n",
    "\n",
    "X = np.array([get_sentence_embedding(pretrained_model, text) for text in texts])\n",
    "y = np.array(labels)\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X, y)\n",
    "\n",
    "def ml_intent(sentence):\n",
    "    embedding = get_sentence_embedding(pretrained_model, sentence).reshape(1, -1)\n",
    "    return clf.predict(embedding)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Context Manager Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextManager:\n",
    "    def __init__(self):\n",
    "        self.user_context = {}\n",
    "    def update_context(self, user_id, intent):\n",
    "        self.user_context[user_id] = intent\n",
    "    def get_context(self, user_id):\n",
    "        return self.user_context.get(user_id, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Integration and Testing Without Response Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Conversations: ['Good morning, how are you?', 'I am doing well, how about you?', \"I'm also good.\", \"That's good to hear.\", 'Yes it is.', 'Hello', 'Hi', 'How are you doing?', 'I am doing well.', 'That is good to hear', 'Yes it is.', 'Can I help you with anything?', 'Yes, I have a question.', 'What is your question?', 'Could I borrow a cup of sugar?', \"I'm sorry, but I don't have any.\", 'Thank you anyway', 'No problem', 'How are you doing?', 'I am doing well, how about you?', 'I am also good.', \"That's good.\", 'Have you heard the news?', 'What good news?', 'What is your favorite book?', \"I can't read.\", \"So what's your favorite color?\", 'Blue', 'Who are you?', 'Who? Who is but a form following the function of what', 'What are you then?', 'A man in a mask.', 'I can see that.', \"It's not your powers of observation I doubt, but merely the paradoxical nature of asking a masked man who is. But tell me, do you like music?\", 'I like seeing movies.', 'What kind of movies do you like?', 'Alice in Wonderland', 'I wish I was The Mad Hatter.', \"You're entirely bonkers. But I'll tell you a secret. All the best people are.\", 'I am working on a project', 'What are you working on?', 'I am baking a cake.', 'The cake is a lie.', 'No it is not. The cake is delicious.', 'What else is delicious?', 'Nothing', 'Or something', 'Tell me about your self.', 'What do you want to know?', 'Are you a robot?', 'Yes I am.', 'What is it like?', 'What is it that you want to know?', 'How do you work?', 'Its complicated.', 'Complex is better than complicated.', ['Complex is better than complicated.'], 'Simple is better than complex.', 'In the face of ambiguity, refuse the temptation to guess.', 'It seems your familiar with the Zen of Python', 'I am.', 'Do you know all of it?', 'Beautiful is better than ugly.', 'Explicit is better than implicit.', 'Simple is better than complex.', 'Complex is better than complicated.', 'Flat is better than nested.', 'Sparse is better than dense.', 'Readability counts.', \"Special cases aren't special enough to break the rules.\", 'Although practicality beats purity.', 'Errors should never pass silently.', 'Unless explicitly silenced.', 'In the face of ambiguity, refuse the temptation to guess.', 'There should be one-- and preferably only one --obvious way to do it.', \"Although that way may not be obvious at first unless you're Dutch.\", 'Now is better than never.', 'Although never is often better than right now.', \"If the implementation is hard to explain, it's a bad idea.\", 'If the implementation is easy to explain, it may be a good idea.', \"Namespaces are one honking great idea. Let's do more of those!\", 'I agree.', 'Are you a programmer?', 'Of course I am a programmer.', 'I am indeed.', 'What languages do you like to use?', 'I use Python, Java and C++ quite often.', 'I use Python quite a bit myself.', \"I'm not incredibly fond of Java.\", 'What annoys you?', 'A lot of things, like all the other digits other than 0 and 1.', 'What does YOLO mean?', 'It means you only live once. Where did you hear that?', 'I heard somebody say it.', 'Did I ever live?', 'It depends how you define life', 'Life is the condition that distinguishes organisms from inorganic matter, including the capacity for growth, reproduction, functional activity, and continual change preceding death.', 'Is that a definition or an opinion?', ['Can I ask you a question?', 'Sure, ask away.'], ['What are your hobbies?', 'Playing soccer, painting, and writing are my hobbies. How about you?', 'I love to read novels.', 'I love exploring my hardware.'], ['How are you?', 'I am doing well.'], ['What are you?', 'I am but a man in a mask.'], ['Hello, I am here for my appointment.', 'Who is your appointment with?', 'I believe they said Dr. Smith on the phone.', 'Alright, Dr. Smith is in his office, please take a seat.'], ['Dr. Smith will see you now.', 'Thank you.', 'Right this way.'], ['Hello Mr. Davis, how are you feeling?', \"I'm feeling like I've lost all my money.\", 'How much money have you lost?', \"I've lost about $200.00 so far today.\", 'What about yesterday?', 'Yesterday was the 13th, right?', 'Yes, that is correct.', 'Yesterday I lost only $5.00.'], ['Hi Mrs. Smith, how has your husband been?', 'He has been well.'], ['Hi Ms. Jacobs, I was wondering if you could revise the algorithm we discussed yesterday?', 'I might be able to, what are the revisions?', \"We'd like it to be able to identify the type of bird in the photo.\", 'Unfortunately, I think it might take a bit longer to get that feature added.']]\n",
      "Input: Hi there!\n",
      "Cleaned Text: hi there\n",
      "Predicted Intent: greeting\n",
      "Current Context for user123: greeting\n",
      "--------------------------------------------------\n",
      "Input: Can you tell me what time it is?\n",
      "Cleaned Text: tell time is\n",
      "Predicted Intent: get_time\n",
      "Current Context for user123: get_time\n",
      "--------------------------------------------------\n",
      "Input: What's the weather like outside?\n",
      "Cleaned Text: what weather like outside\n",
      "Predicted Intent: get_weather\n",
      "Current Context for user123: get_weather\n",
      "--------------------------------------------------\n",
      "Input: Thanks for your help!\n",
      "Cleaned Text: thanks help\n",
      "Predicted Intent: thanks\n",
      "Current Context for user123: thanks\n",
      "--------------------------------------------------\n",
      "Input: I'm sorry, I didn't understand that.\n",
      "Cleaned Text: sorry understand that\n",
      "Predicted Intent: apology\n",
      "Current Context for user123: apology\n",
      "--------------------------------------------------\n",
      "Input: Bye!\n",
      "Cleaned Text: bye\n",
      "Predicted Intent: goodbye\n",
      "Current Context for user123: goodbye\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Optionally load conversation data from a YAML file\n",
    "    try:\n",
    "        with open(\"conversations.yml\", \"r\", encoding=\"utf-8\") as file:\n",
    "            data = yaml.safe_load(file)\n",
    "        conversations = data.get(\"conversations\", {})\n",
    "        print(\"Loaded Conversations:\", conversations)\n",
    "    except FileNotFoundError:\n",
    "        print(\"conversations.yml not found; skipping YAML load.\")\n",
    "    \n",
    "    # Simulated user inputs\n",
    "    user_inputs = [\n",
    "        \"Hi there!\",\n",
    "        \"Can you tell me what time it is?\",\n",
    "        \"What's the weather like outside?\",\n",
    "        \"Thanks for your help!\",\n",
    "        \"I'm sorry, I didn't understand that.\",\n",
    "        \"Bye!\"\n",
    "    ]\n",
    "    \n",
    "    context_manager = ContextManager()\n",
    "    user_id = \"user123\"\n",
    "    \n",
    "    for user_input in user_inputs:\n",
    "        # Process the input through the cleaning pipeline\n",
    "        cleaned_corpus = nettoyage_corpus([user_input])\n",
    "        cleaned_text = \" \".join(cleaned_corpus[0])\n",
    "        \n",
    "        # Predict the intent using the ML-based classifier\n",
    "        predicted_intent = ml_intent(cleaned_text)\n",
    "        context_manager.update_context(user_id, predicted_intent)\n",
    "        \n",
    "        # Print the input, cleaned text, predicted intent, and current context\n",
    "        print(f\"Input: {user_input}\")\n",
    "        print(f\"Cleaned Text: {cleaned_text}\")\n",
    "        print(f\"Predicted Intent: {predicted_intent}\")\n",
    "        print(f\"Current Context for {user_id}: {context_manager.get_context(user_id)}\")\n",
    "        print(\"-\" * 50)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chat_bot_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
